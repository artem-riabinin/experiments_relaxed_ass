{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Set parameters\n",
    "learning_rate = 0.05\n",
    "batch_size = 128\n",
    "epochs = 300\n",
    "input_dim = 784  # 28x28 images from Fashion-MNIST\n",
    "hidden_dim = 64\n",
    "output_dim = 10  # 10 classes in Fashion-MNIST\n",
    "c_1 = 0.1\n",
    "c_11 = 0.01\n",
    "num_seeds = 3  # Number of different seeds\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def predict(model, inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "    return outputs\n",
    "\n",
    "# Load dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Store results for different seeds\n",
    "all_losses = []\n",
    "all_gradients = []\n",
    "all_inner_products = []\n",
    "all_inner_products2 = []\n",
    "\n",
    "# Training function\n",
    "def train(seed):\n",
    "    # Fix seed\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = MLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch_losses = []\n",
    "    epoch_gradients = []\n",
    "    epoch_params = []\n",
    "    losses = []\n",
    "    gradients_norm = []\n",
    "    inputs_saved = []\n",
    "    targets_saved = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch', epoch)\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.view(-1, 28 * 28).to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            if batch_idx == len(train_loader) - 2:\n",
    "                gradients = [param.grad.clone() for param in model.parameters()]\n",
    "                epoch_gradients.append(gradients)\n",
    "                epoch_losses.append(loss.item())\n",
    "                params = [param.clone().detach() for param in model.parameters()]\n",
    "                epoch_params.append(params)\n",
    "                inputs_saved.append(inputs)\n",
    "                targets_saved.append(targets)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}\")\n",
    "\n",
    "        # Compute loss and gradient norm\n",
    "        #total_loss = 0.0\n",
    "        #num_samples = len(train_loader.dataset)\n",
    "        #for inputs, targets in train_loader:\n",
    "        #    inputs, targets = inputs.view(-1, 28 * 28).to(device), targets.to(device)\n",
    "        #    outputs = model(inputs)\n",
    "        #    total_loss += criterion(outputs, targets) * inputs.shape[0]\n",
    "        #total_loss /= num_samples\n",
    "        #grads = torch.autograd.grad(total_loss, model.parameters(), create_graph=False)\n",
    "        #losses.append(total_loss.detach())\n",
    "        #gradients_norm.append(torch.norm(torch.cat([p.detach().view(-1) for p in grads])))\n",
    "\n",
    "        #print(f\"Seed {seed}, Epoch {epoch+1}/{epochs}, Loss: {losses[-1]}, Gradient Norm: {gradients_norm[-1]}\")\n",
    "\n",
    "    # Compute inner products\n",
    "    x_star = [param.clone().detach() for param in epoch_params[-1]]\n",
    "    inner_products = []\n",
    "    inner_products2 = []\n",
    "    x_star_vector = torch.cat([x.view(-1) for x in x_star])\n",
    "    for epoch in range(epochs):\n",
    "        grad_vector = torch.cat([g.view(-1) for g in epoch_gradients[epoch]])\n",
    "        param_vector = torch.cat([p.view(-1) for p in epoch_params[epoch]])\n",
    "        inner_product = torch.dot(grad_vector, param_vector - x_star_vector) - c_1 * epoch_losses[epoch] + c_1 * criterion(predict(model, inputs_saved[epoch]), targets_saved[epoch])\n",
    "        inner_products.append(inner_product.item())\n",
    "        inner_product2 = torch.dot(grad_vector, param_vector - x_star_vector) - c_11 * torch.norm(grad_vector) ** 2\n",
    "        inner_products2.append(inner_product2.item())\n",
    "\n",
    "    return losses, gradients_norm, inner_products, inner_products2\n",
    "\n",
    "# Run training for different seeds\n",
    "for seed in range(num_seeds):\n",
    "    losses, gradients_norm, inner_products, inner_products2 = train(seed)\n",
    "    all_losses.append(losses)\n",
    "    all_gradients.append(gradients_norm)\n",
    "    all_inner_products.append(inner_products)\n",
    "    all_inner_products2.append(inner_products2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy arrays\n",
    "all_losses = np.array(all_losses)\n",
    "all_gradients = np.array(all_gradients)\n",
    "all_inner_products = np.array(all_inner_products)\n",
    "all_inner_products2 = np.array(all_inner_products2)\n",
    "\n",
    "\n",
    "#all_losses = np.array([[loss.cpu().numpy() if isinstance(loss, torch.Tensor) else loss for loss in inner_list] for inner_list in all_losses])\n",
    "#all_gradients = np.array([[grad.cpu().numpy() if isinstance(grad, torch.Tensor) else grad for grad in inner_list] for inner_list in all_gradients])\n",
    "#all_inner_products = np.array([[inner_product.cpu().numpy() if isinstance(inner_product, torch.Tensor) else inner_product for inner_product in inner_list] for inner_list in all_inner_products])\n",
    "#all_inner_products2 = np.array([[inner_product2.cpu().numpy() if isinstance(inner_product2, torch.Tensor) else inner_product2 for inner_product2 in inner_list] for inner_list in all_inner_products2])\n",
    "\n",
    "\n",
    "# Compute statistics\n",
    "mean_inner_products = np.mean(all_inner_products, axis=0)\n",
    "min_inner_products = np.min(all_inner_products, axis=0)\n",
    "max_inner_products = np.max(all_inner_products, axis=0)\n",
    "\n",
    "mean_inner_products2 = np.mean(all_inner_products2, axis=0)\n",
    "min_inner_products2 = np.min(all_inner_products2, axis=0)\n",
    "max_inner_products2 = np.max(all_inner_products2, axis=0)\n",
    "\n",
    "# Plotting the results\n",
    "plt.style.use('default')\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Corrected subplot setup\n",
    "\n",
    "# Plot 1: First case\n",
    "axs[0].plot(mean_inner_products, label='Mean', linewidth=1.5)\n",
    "axs[0].fill_between(range(len(mean_inner_products)), min_inner_products, max_inner_products,\n",
    "                    alpha=0.3, label='Min-Max')\n",
    "axs[0].set_title(r'$\\left\\langle \\nabla f_{\\xi}(x^k), x^k - x^K \\right\\rangle - c_1 \\left(f_{\\xi}(x^k) - f_{\\xi}(x^K)\\right)$, $c_1=0.1$')\n",
    "axs[0].set_xlabel('Epoch $k$')\n",
    "axs[0].set_ylabel(r'$\\left\\langle \\nabla f_{\\xi}(x^k), x^k - x^K \\right\\rangle - c_1 \\left(f_{\\xi}(x^k) - f_{\\xi}(x^K)\\right)$')\n",
    "axs[0].legend(loc='upper right')\n",
    "axs[0].set_ylim(-0.55, 1.1)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot 2: Second case\n",
    "axs[1].plot(mean_inner_products2, label='Mean', linewidth=1.5)\n",
    "axs[1].fill_between(range(len(mean_inner_products2)), min_inner_products2, max_inner_products2,\n",
    "                    alpha=0.3, label='Min-Max')\n",
    "axs[1].set_title(r'$\\left\\langle \\nabla f_{\\xi}(x^k), x^k - x^K \\right\\rangle - c_1 \\|\\nabla f_{\\xi}(x^k)\\|^2$, $c_1=0.01$')\n",
    "axs[1].set_xlabel('Epoch $k$')\n",
    "axs[1].set_ylabel(r'$\\left\\langle \\nabla f_{\\xi}(x^k), x^k - x^K \\right\\rangle - c_1 \\|\\nabla f_{\\xi}(x^k)\\|^2$')\n",
    "axs[1].legend(loc='upper right')\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Layout and saving\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plot2.pdf\", format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Find c_2 (for both cases)\n",
    "max_c2 = -np.min(all_inner_products)\n",
    "print(\"Max c_2 for the first case:\", max_c2)\n",
    "max_c2_2 = -np.min(all_inner_products2)\n",
    "print(\"Max c_2 for the second case:\", max_c2_2)\n",
    "\n",
    "# Find c_2\n",
    "max_c2 = -np.min(all_inner_products)\n",
    "print(\"Max c_2 for the first case:\", max_c2)\n",
    "max_c2_2 = -np.min(all_inner_products2)\n",
    "print(\"Max c_2 for the second case:\", max_c2_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
